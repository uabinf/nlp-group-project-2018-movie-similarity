{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import re\n",
    "import string\n",
    "import glob\n",
    "import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# UNCOMMENT TO GET CONVERGENCE LOGS\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.test.utils import datapath\n",
    "from nltk import word_tokenize, WordNetLemmatizer, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE PROCESSING \n",
    "def get_subtitle_fns(folder_fn_regex='srt/*'):\n",
    "    return glob.glob(folder_fn_regex)\n",
    "\n",
    "def get_subtitle_names(folder_fn_regex='srt/*'):\n",
    "    filenames = glob.glob(folder_fn_regex)\n",
    "    return [fn[4:-4] for fn in filenames] # remove srt\\\\ and .srt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SUBTITLE PREPROCESSING\n",
    "\n",
    "# expected subtitle format:\n",
    "# numeric timestamp index\n",
    "# timestamp\n",
    "# subtitles on >= one line\n",
    "# linebreak\n",
    "\n",
    "# end of a subtitle line is expected to be a full word (i.e., not hyphenated)\n",
    "\n",
    "# subtitles may have the following qualities:\n",
    "# 1. markup (bold, italics, font colour) is between <> \n",
    "# e.g., <font color=\"#00ff00\"></font> \n",
    "# 2. begin with hyphens\n",
    "# 3. contain punctuation\n",
    "# 4. begin (subtitle index 0) and end with subtitle credits of the same format \n",
    "# 5. end with \"The End\"\n",
    "# 6. contain credits at the start of the subtitles \n",
    "\n",
    "def get_subtitle_corpus(subtitle_fns):\n",
    "    return [preprocess_movie_by_subs(s_fn) for s_fn in subtitle_fns]\n",
    "\n",
    "def preprocess_movie_by_subs(subtitle_fn):\n",
    "    return preprocess_subs(get_raw_subs(subtitle_fn))\n",
    "\n",
    "def get_raw_subs(filename):\n",
    "    subs = []\n",
    "    sub_data = []\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:          \n",
    "            # collect sub lines of some timestamp\n",
    "            if line != '\\n':\n",
    "                sub_data.append(line[:-1]) # remove newline\n",
    "                continue\n",
    "            \n",
    "            sub_idx = sub_data[0]\n",
    "            is_sub_author_credits = (sub_idx == '0')\n",
    "            if is_sub_author_credits: \n",
    "                sub_data = []\n",
    "                continue\n",
    "            \n",
    "            # add sub line to all subs\n",
    "            for i, sub_line in enumerate(sub_data):            \n",
    "                if i > 1: # index 0 and 1 are sub index and timestamp\n",
    "                    subs.append(sub_line)\n",
    "                \n",
    "            sub_data = []\n",
    "    \n",
    "    last_sub_not_included = (sub_data != [])\n",
    "    if last_sub_not_included:\n",
    "        for i, sub_line in enumerate(sub_data):            \n",
    "            if i > 1: # index 0 and 1 are sub index and timestamp\n",
    "                subs.append(sub_line)\n",
    "    \n",
    "    return subs\n",
    "\n",
    "def preprocess_subs(raw):\n",
    "    # remove markup\n",
    "    markup = '<[^>]*>'\n",
    "    subs = [re.sub(markup, '', r) for r in raw]\n",
    "    \n",
    "    # tokenisation case-folding\n",
    "    subs = [word_tokenize(s.lower()) for s in subs]\n",
    "    \n",
    "    # remove stopwords and punctuation\n",
    "    stops = set(stopwords.words('english'))\n",
    "    stops = stops.union(string.punctuation)\n",
    "    subs = [[t for t in sub if t not in stops] for sub in subs]\n",
    "    \n",
    "    # lemmatize\n",
    "    pos = get_pos_tags(subs) # different POS lemmatize differently\n",
    "    subs = [t for sub in subs for t in sub] # flatten\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    subs = [lmtzr.lemmatize(s, pos[i]) for i, s in enumerate(subs)]\n",
    "        \n",
    "    return subs\n",
    "\n",
    "def get_pos_tags(subs):\n",
    "    pos = [pos_tag(sub) for sub in subs]\n",
    "    pos = [get_wordnet_pos(tag) for sub in pos for t, tag in sub] # flatten\n",
    "    return pos\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else: # default POS in WordNetLemmatizer is noun\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def get_vocabulary(bag_of_words):\n",
    "    return set(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -0.8803\n",
      "[([(0.0028490152, 'spot'),\n",
      "   (0.0028490152, 'write'),\n",
      "   (0.0028490152, 'excited'),\n",
      "   (0.0028490152, 'gather'),\n",
      "   (0.0028490152, 'tomorrow'),\n",
      "   (0.0028490152, 'studio'),\n",
      "   (0.0028490152, 'rob'),\n",
      "   (0.0028490152, 'often'),\n",
      "   (0.0028490152, 'perform'),\n",
      "   (0.0028490152, 'splendid'),\n",
      "   (0.002849015, 'lovely'),\n",
      "   (0.002849015, 'ground'),\n",
      "   (0.002849015, 'happy'),\n",
      "   (0.002849015, 'design'),\n",
      "   (0.002849015, 'beloved'),\n",
      "   (0.002849015, 'scare'),\n",
      "   (0.002849015, 'success'),\n",
      "   (0.002849015, 'truth'),\n",
      "   (0.002849015, 'shield'),\n",
      "   (0.002849015, 'ghibli')],\n",
      "  -0.18605529583197486),\n",
      " ([(0.055583388, 'witch'),\n",
      "   (0.03420439, 'ok'),\n",
      "   (0.025298174, 'shop'),\n",
      "   (0.023518305, 'demon'),\n",
      "   (0.023516394, 'war'),\n",
      "   (0.018171843, 'kind'),\n",
      "   (0.016392307, \"ma'am\"),\n",
      "   (0.016391154, 'ever'),\n",
      "   (0.014608616, 'report'),\n",
      "   (0.011045802, 'henchman'),\n",
      "   (0.011045258, 'clean'),\n",
      "   (0.009265018, 'trouble'),\n",
      "   (0.009264548, 'else'),\n",
      "   (0.009264082, 'family'),\n",
      "   (0.009264022, 'tacky'),\n",
      "   (0.0074842367, 'enemy'),\n",
      "   (0.007483676, 'free'),\n",
      "   (0.0074836626, 'room'),\n",
      "   (0.007483587, 'rest'),\n",
      "   (0.0074834893, 'course')],\n",
      "  -0.350221733332541),\n",
      " ([(0.0028870995, 'witch'),\n",
      "   (0.0028779558, 'ok'),\n",
      "   (0.0028658053, 'demon'),\n",
      "   (0.0028647182, 'war'),\n",
      "   (0.002860909, 'shop'),\n",
      "   (0.002859217, 'kind'),\n",
      "   (0.0028585524, 'woman'),\n",
      "   (0.0028568662, 'report'),\n",
      "   (0.0028567088, 'clean'),\n",
      "   (0.002854762, 'ever'),\n",
      "   (0.0028539293, 'rest'),\n",
      "   (0.002853797, 'men'),\n",
      "   (0.0028534478, 'else'),\n",
      "   (0.0028533726, 'family'),\n",
      "   (0.002853261, 'course'),\n",
      "   (0.0028531828, 'tacky'),\n",
      "   (0.0028529393, 'breakfast'),\n",
      "   (0.0028528434, 'free'),\n",
      "   (0.0028527363, 'henchman'),\n",
      "   (0.0028525307, 'mark')],\n",
      "  -0.3648143055548135),\n",
      " ([(0.034599386, 'men'),\n",
      "   (0.03246407, 'woman'),\n",
      "   (0.021785699, 'toki'),\n",
      "   (0.017513946, 'tree'),\n",
      "   (0.017513366, 'demon'),\n",
      "   (0.017513236, 'death'),\n",
      "   (0.015378426, 'wood'),\n",
      "   (0.015378229, 'mountain'),\n",
      "   (0.0132432645, 'wish'),\n",
      "   (0.013242037, 'quickly'),\n",
      "   (0.013241981, 'west'),\n",
      "   (0.013241751, 'attack'),\n",
      "   (0.0132417325, 'mark'),\n",
      "   (0.011106125, 'strength'),\n",
      "   (0.01110608, 'world'),\n",
      "   (0.011106065, 'arm'),\n",
      "   (0.011105797, 'grow'),\n",
      "   (0.008970864, 'heavy'),\n",
      "   (0.008970764, 'fool'),\n",
      "   (0.008970559, 'form')],\n",
      "  -0.4706104541662893),\n",
      " ([(0.045986976, 'okay'),\n",
      "   (0.028201185, 'hmm'),\n",
      "   (0.023119317, 'gold'),\n",
      "   (0.023119083, 'welcome'),\n",
      "   (0.021851124, 'fish'),\n",
      "   (0.02057906, 'job'),\n",
      "   (0.019308506, 'river'),\n",
      "   (0.018038593, 'mom'),\n",
      "   (0.015498868, 'jerk'),\n",
      "   (0.015497969, 'bath'),\n",
      "   (0.0142277945, 'heave'),\n",
      "   (0.01295715, 'guest'),\n",
      "   (0.011686878, 'boiler'),\n",
      "   (0.011686849, 'pig'),\n",
      "   (0.011686408, \"ma'am\"),\n",
      "   (0.010417401, 'boat'),\n",
      "   (0.010416577, 'granny'),\n",
      "   (0.010416376, 'stink'),\n",
      "   (0.009146842, 'ship'),\n",
      "   (0.009146615, 'school')],\n",
      "  -3.029915849944115)]\n",
      "Wall time: 5.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TOPIC MODELLING\n",
    "def load_lda(filename):\n",
    "    return LdaModel.load(filename)\n",
    "\n",
    "movies_corpus = get_subtitle_corpus(get_subtitle_fns()) \n",
    "dictionary = Dictionary(movies_corpus)\n",
    "\n",
    "# remove low information words based on how frequently they appear in \n",
    "    # the intra-document and inter-document levels\n",
    "    # 1. sparsely occurring words for each document; and \n",
    "    # 2. words frequent over the WHOLE collection\n",
    "# remove words appearing in <no_below or >no_above% movies\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.5) # TODO adjust\n",
    "\n",
    "movies_corpus = [dictionary.doc2bow(movie) for movie in movies_corpus] # vect\n",
    "\n",
    "num_topics = 5 # paper used 55\n",
    "chunksize = 2000 # default\n",
    "passes = 20 # TODO adjust this ------ GRID SEARCH?\n",
    "iterations = 400 # TODO adjust this --- passes and iterations must be adjusted to allow convergence\n",
    "eval_every = None # apparently evaluating model perplexity takes too much time?\n",
    "\n",
    "lda = LdaModel(movies_corpus, \n",
    "               id2word=dictionary,\n",
    "               chunksize=chunksize,\n",
    "               passes=passes,\n",
    "               iterations=iterations,\n",
    "               num_topics=num_topics,\n",
    "               eval_every=eval_every)\n",
    "\n",
    "lda.save(datapath('test')) # give absolute path\n",
    "\n",
    "top_topics = lda.top_topics(movies_corpus, topn=20)\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "\n",
    "print('Average topic coherence: %.4f' % avg_topic_coherence)\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ponyo', 1.0), ('spirited away', 1.0), (\"Howl's Moving Castle 2004\", 0.0), ('princess mononoke', 0.0)]\n",
      "***** HOWL'S MOVING CASTLE 2004 *****\n",
      "[(\"Howl's Moving Castle 2004\", 1.0), ('ponyo', 0.0), ('princess mononoke', 0.0), ('spirited away', 0.0)]\n",
      "\n",
      "***** PONYO *****\n",
      "[('ponyo', 1.0), ('spirited away', 1.0), (\"Howl's Moving Castle 2004\", 0.0), ('princess mononoke', 0.0)]\n",
      "\n",
      "***** PRINCESS MONONOKE *****\n",
      "[('princess mononoke', 1.0), (\"Howl's Moving Castle 2004\", 0.0), ('ponyo', 0.0), ('spirited away', 0.0)]\n",
      "\n",
      "***** SPIRITED AWAY *****\n",
      "[('ponyo', 1.0), ('spirited away', 1.0), (\"Howl's Moving Castle 2004\", 0.0), ('princess mononoke', 0.0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# topic similarity\n",
    "# cosine similarity between topic components \n",
    "\n",
    "def get_movie_similarity(index, subtitle_fn):    \n",
    "    movie = preprocess_movie_by_subs(subtitle_fn)\n",
    "    sims = index[lda[dictionary.doc2bow(movie)]]\n",
    "    return sort_desc_movie_similarities(sims)\n",
    "\n",
    "def get_movie_similarities(lda, index, movies_corpus):\n",
    "    return [index[lda[movie]] for movie in movies_corpus]\n",
    "\n",
    "def sort_desc_movie_similarities(sims):\n",
    "    movie_names = get_subtitle_names()\n",
    "    return sorted(zip(movie_names, sims), key=lambda sim: -sim[1])\n",
    "\n",
    "index = MatrixSimilarity(lda[movies_corpus]) \n",
    "# if memory is an issue, use Similarity: https://radimrehurek.com/gensim/similarities/docsim.html\n",
    "# \"For example, a corpus of one million documents would require 2GB of RAM in a 256-dimensional LSI space, when used with this class.\"\n",
    "# https://radimrehurek.com/gensim/tut3.html\n",
    "\n",
    "print(get_movie_similarity(index, \"srt/spirited away.srt\"))\n",
    "\n",
    "movie_names = get_subtitle_names()\n",
    "for movie, sims in zip(movie_names, get_movie_similarities(lda, index, movies_corpus)):\n",
    "    print(\"*****\", movie.upper(), \"*****\")\n",
    "    print(sort_desc_movie_similarities(sims))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise total similarity matrix \n",
    "# white = 1, black = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Howl's Moving Castle 2004\", 'ponyo', 'princess mononoke', 'spirited away']\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
