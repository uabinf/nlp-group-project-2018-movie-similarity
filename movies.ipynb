{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "from nltk import TweetTokenizer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBTITLE FILENAMES\n",
    "\n",
    "folder_fn = 'srt/'\n",
    "subtitle_fns = [\"howl's moving castle\", \"ponyo\", \n",
    "                \"princess mononoke\", \"spirited away\"]\n",
    "fn_ext = '.srt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBTITLE PREPROCESSING\n",
    "\n",
    "# expected subtitle format:\n",
    "# numeric timestamp index\n",
    "# timestamp\n",
    "# subtitles on >= one line\n",
    "# linebreak\n",
    "\n",
    "# end of a subtitle line is expected to be a full word (i.e., not hyphenated)\n",
    "\n",
    "# subtitles may have the following qualities:\n",
    "# 1. markup (bold, italics, font colour) is between <> \n",
    "# e.g., <font color=\"#00ff00\"></font> \n",
    "# 2. begin with hyphens\n",
    "# 3. contain punctuation\n",
    "# 4. begin (subtitle index 0) and end with subtitle credits of the same format \n",
    "# 5. end with \"The End\"\n",
    "# 6. contain credits at the start of the subtitles \n",
    "\n",
    "def get_raw_subs(filename):\n",
    "    subs = []\n",
    "    sub_data = []\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:          \n",
    "            # collect sub lines of some timestamp\n",
    "            if line != '\\n':\n",
    "                sub_data.append(line[:-1]) # remove newline\n",
    "                continue\n",
    "            \n",
    "            is_sub_author_credits = (sub_data[0] == '0')\n",
    "            if is_sub_author_credits: \n",
    "                sub_data = []\n",
    "                continue\n",
    "            \n",
    "            # add sub line to all subs\n",
    "            for i, sub_line in enumerate(sub_data):            \n",
    "                if i > 1: # index 0 and 1 are sub index and timestamp\n",
    "                    subs.append(sub_line)\n",
    "                \n",
    "            sub_data = []\n",
    "    \n",
    "    return subs\n",
    "\n",
    "def preprocess_subs(raw):\n",
    "    subs = ' '.join(raw)\n",
    "    \n",
    "    # remove markup\n",
    "    markup = '<[^>]*>'\n",
    "    subs = re.sub(markup, '', subs)\n",
    "    \n",
    "    # tokenisation case-folding\n",
    "    subs = subs.lower()\n",
    "    subs = TweetTokenizer().tokenize(subs) # tokens include contractions\n",
    "    \n",
    "    # remove stopwords and punctuation\n",
    "    stops = set(stopwords.words('english'))\n",
    "    extra_stops = {'...', '... ...'}\n",
    "    stops = stops.union(extra_stops)\n",
    "    subs = [t for t in subs if t not in stops and t not in string.punctuation]\n",
    "\n",
    "    # TODO\n",
    "    # remove low information words based on how frequently they appear in \n",
    "    # the intra-document and inter-document levels\n",
    "    # 1. sparsely occurring words for each document; and \n",
    "    # 2. words frequent over the WHOLE collection\n",
    "    \n",
    "    # lemmatize\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    subs = [lmtzr.lemmatize(s) for s in subs]\n",
    "    \n",
    "    return subs\n",
    "\n",
    "def get_vocabulary(bag_of_words):\n",
    "    return set(bag_of_words)\n",
    "\n",
    "for sfn in subtitle_fns:\n",
    "    filename = folder_fn + sfn + fn_ext\n",
    "    raw = get_raw_subs(filename)\n",
    "    bag_of_words = preprocess_subs(raw)\n",
    "    vocabulary = get_vocabulary(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modelling\n",
    "\n",
    "# use gensim \n",
    "# collapsed gibbs sampling to approximate posterior \n",
    "\n",
    "# paper used T = 55 topics to be extracted \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic similarity\n",
    "\n",
    "# cosine similarity between topic components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise total similarity matrix \n",
    "# white = 1, black = 0 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
